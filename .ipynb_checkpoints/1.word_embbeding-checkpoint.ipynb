{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/#\n",
    "# !curl -O \"https://nlp.stanford.edu/data/glove.6B.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import one_hot, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Flatten, Dense, Input\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define documents\n",
    "docs = np.array(['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.'])\n",
    "# define class labels\n",
    "labels = np.array([1,1,1,1,1,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Well done!', 'Good work', 'Great effort', 'nice work',\n",
       "        'Excellent!', 'Weak', 'Poor effort!', 'not good', 'poor work',\n",
       "        'Could have done better.'],\n",
       "       dtype='|S23'),\n",
       " array(['not good', 'Great effort', 'Could have done better.', 'Excellent!',\n",
       "        'nice work', 'Poor effort!'],\n",
       "       dtype='|S23'),\n",
       " array([0, 1, 0, 1, 1, 0]))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_train, docs_test, labels_train, labels_test = train_test_split(docs, labels, test_size=0.33, random_state=42)\n",
    "docs, docs_train, labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 2], [3, 1], [7, 4], [8, 1], [9], [10], [5, 4], [11, 3], [5, 1], [12, 13, 2, 14]] 10\n"
     ]
    }
   ],
   "source": [
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "print(encoded_docs), len(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6  2  0  0]\n",
      " [ 3  1  0  0]\n",
      " [ 7  4  0  0]\n",
      " [ 8  1  0  0]\n",
      " [ 9  0  0  0]\n",
      " [10  0  0  0]\n",
      " [ 5  4  0  0]\n",
      " [11  3  0  0]\n",
      " [ 5  1  0  0]\n",
      " [12 13  2 14]]\n"
     ]
    }
   ],
   "source": [
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)\n",
    "def encode_input(d):\n",
    "    return pad_sequences(t.texts_to_sequences(np.array([d])), maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_22 (Embedding)     (None, 4, 8)              120       \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 153\n",
      "Trainable params: 153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_23 (Embedding)     (None, 4, 8)              120       \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 153\n",
      "Trainable params: 153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "inputs = Input(shape=(max_length,))\n",
    "embbeded_layer = Embedding(vocab_size, 8, input_length=max_length)(inputs)\n",
    "flatten_layer = Flatten()(embbeded_layer)\n",
    "dense_layer = Dense(1, activation='sigmoid')(flatten_layer)\n",
    "model = Model(inputs=inputs,outputs=dense_layer)\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 1s 52ms/step - loss: 0.7010 - acc: 0.2000\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 0s 153us/step - loss: 0.6993 - acc: 0.2000\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 0s 146us/step - loss: 0.6976 - acc: 0.2000\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 0s 152us/step - loss: 0.6960 - acc: 0.4000\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 0s 148us/step - loss: 0.6943 - acc: 0.4000\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 0s 146us/step - loss: 0.6927 - acc: 0.4000\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 0s 139us/step - loss: 0.6911 - acc: 0.4000\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 0s 160us/step - loss: 0.6894 - acc: 0.4000\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 0s 161us/step - loss: 0.6878 - acc: 0.6000\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 0s 136us/step - loss: 0.6862 - acc: 0.6000\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 0s 126us/step - loss: 0.6846 - acc: 0.7000\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 0s 290us/step - loss: 0.6830 - acc: 0.7000\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 0s 241us/step - loss: 0.6814 - acc: 0.7000\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 0s 294us/step - loss: 0.6798 - acc: 0.7000\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 0s 158us/step - loss: 0.6782 - acc: 0.7000\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 0s 205us/step - loss: 0.6766 - acc: 0.7000\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 0s 140us/step - loss: 0.6750 - acc: 0.7000\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 0s 313us/step - loss: 0.6734 - acc: 0.7000\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 0s 129us/step - loss: 0.6717 - acc: 0.7000\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 0s 235us/step - loss: 0.6701 - acc: 0.8000\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 0s 128us/step - loss: 0.6685 - acc: 0.8000\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 0s 411us/step - loss: 0.6669 - acc: 0.8000\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 0s 148us/step - loss: 0.6652 - acc: 0.8000\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 0s 242us/step - loss: 0.6636 - acc: 0.8000\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 0s 199us/step - loss: 0.6620 - acc: 0.8000\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 0s 155us/step - loss: 0.6603 - acc: 0.8000\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 0s 275us/step - loss: 0.6586 - acc: 0.8000\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 0s 185us/step - loss: 0.6570 - acc: 0.8000\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 0s 253us/step - loss: 0.6553 - acc: 0.8000\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 0s 527us/step - loss: 0.6536 - acc: 0.8000\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 0s 174us/step - loss: 0.6519 - acc: 0.8000\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - 0s 148us/step - loss: 0.6502 - acc: 0.8000\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - 0s 170us/step - loss: 0.6485 - acc: 0.8000\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - 0s 185us/step - loss: 0.6468 - acc: 0.8000\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - 0s 183us/step - loss: 0.6451 - acc: 0.8000\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - 0s 149us/step - loss: 0.6434 - acc: 0.8000\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - 0s 170us/step - loss: 0.6416 - acc: 0.8000\n",
      "Epoch 38/100\n",
      "10/10 [==============================] - 0s 146us/step - loss: 0.6399 - acc: 0.8000\n",
      "Epoch 39/100\n",
      "10/10 [==============================] - 0s 335us/step - loss: 0.6381 - acc: 0.8000\n",
      "Epoch 40/100\n",
      "10/10 [==============================] - 0s 516us/step - loss: 0.6363 - acc: 0.8000\n",
      "Epoch 41/100\n",
      "10/10 [==============================] - 0s 268us/step - loss: 0.6345 - acc: 0.8000\n",
      "Epoch 42/100\n",
      "10/10 [==============================] - 0s 215us/step - loss: 0.6327 - acc: 0.8000\n",
      "Epoch 43/100\n",
      "10/10 [==============================] - 0s 292us/step - loss: 0.6309 - acc: 0.8000\n",
      "Epoch 44/100\n",
      "10/10 [==============================] - 0s 183us/step - loss: 0.6291 - acc: 0.8000\n",
      "Epoch 45/100\n",
      "10/10 [==============================] - 0s 244us/step - loss: 0.6273 - acc: 0.8000\n",
      "Epoch 46/100\n",
      "10/10 [==============================] - 0s 294us/step - loss: 0.6254 - acc: 0.8000\n",
      "Epoch 47/100\n",
      "10/10 [==============================] - 0s 214us/step - loss: 0.6236 - acc: 0.8000\n",
      "Epoch 48/100\n",
      "10/10 [==============================] - 0s 621us/step - loss: 0.6217 - acc: 0.8000\n",
      "Epoch 49/100\n",
      "10/10 [==============================] - 0s 254us/step - loss: 0.6198 - acc: 0.8000\n",
      "Epoch 50/100\n",
      "10/10 [==============================] - 0s 215us/step - loss: 0.6180 - acc: 0.8000\n",
      "Epoch 51/100\n",
      "10/10 [==============================] - 0s 581us/step - loss: 0.6161 - acc: 0.8000\n",
      "Epoch 52/100\n",
      "10/10 [==============================] - 0s 351us/step - loss: 0.6142 - acc: 0.8000\n",
      "Epoch 53/100\n",
      "10/10 [==============================] - 0s 364us/step - loss: 0.6123 - acc: 0.8000\n",
      "Epoch 54/100\n",
      "10/10 [==============================] - 0s 216us/step - loss: 0.6103 - acc: 0.8000\n",
      "Epoch 55/100\n",
      "10/10 [==============================] - 0s 231us/step - loss: 0.6084 - acc: 0.8000\n",
      "Epoch 56/100\n",
      "10/10 [==============================] - 0s 182us/step - loss: 0.6064 - acc: 0.8000\n",
      "Epoch 57/100\n",
      "10/10 [==============================] - 0s 434us/step - loss: 0.6045 - acc: 0.8000\n",
      "Epoch 58/100\n",
      "10/10 [==============================] - 0s 271us/step - loss: 0.6025 - acc: 0.8000\n",
      "Epoch 59/100\n",
      "10/10 [==============================] - 0s 642us/step - loss: 0.6005 - acc: 0.8000\n",
      "Epoch 60/100\n",
      "10/10 [==============================] - 0s 344us/step - loss: 0.5986 - acc: 0.8000\n",
      "Epoch 61/100\n",
      "10/10 [==============================] - 0s 165us/step - loss: 0.5966 - acc: 0.8000\n",
      "Epoch 62/100\n",
      "10/10 [==============================] - 0s 255us/step - loss: 0.5946 - acc: 0.8000\n",
      "Epoch 63/100\n",
      "10/10 [==============================] - 0s 535us/step - loss: 0.5925 - acc: 0.8000\n",
      "Epoch 64/100\n",
      "10/10 [==============================] - 0s 253us/step - loss: 0.5905 - acc: 0.8000\n",
      "Epoch 65/100\n",
      "10/10 [==============================] - 0s 633us/step - loss: 0.5885 - acc: 0.8000\n",
      "Epoch 66/100\n",
      "10/10 [==============================] - 0s 279us/step - loss: 0.5865 - acc: 0.8000\n",
      "Epoch 67/100\n",
      "10/10 [==============================] - 0s 989us/step - loss: 0.5844 - acc: 0.8000\n",
      "Epoch 68/100\n",
      "10/10 [==============================] - 0s 764us/step - loss: 0.5823 - acc: 0.8000\n",
      "Epoch 69/100\n",
      "10/10 [==============================] - 0s 293us/step - loss: 0.5803 - acc: 0.9000\n",
      "Epoch 70/100\n",
      "10/10 [==============================] - 0s 633us/step - loss: 0.5782 - acc: 0.9000\n",
      "Epoch 71/100\n",
      "10/10 [==============================] - 0s 819us/step - loss: 0.5761 - acc: 0.9000\n",
      "Epoch 72/100\n",
      "10/10 [==============================] - 0s 273us/step - loss: 0.5740 - acc: 0.9000\n",
      "Epoch 73/100\n",
      "10/10 [==============================] - 0s 365us/step - loss: 0.5719 - acc: 0.9000\n",
      "Epoch 74/100\n",
      "10/10 [==============================] - 0s 800us/step - loss: 0.5698 - acc: 0.9000\n",
      "Epoch 75/100\n",
      "10/10 [==============================] - 0s 310us/step - loss: 0.5677 - acc: 0.9000\n",
      "Epoch 76/100\n",
      "10/10 [==============================] - 0s 299us/step - loss: 0.5656 - acc: 0.9000\n",
      "Epoch 77/100\n",
      "10/10 [==============================] - 0s 355us/step - loss: 0.5635 - acc: 0.9000\n",
      "Epoch 78/100\n",
      "10/10 [==============================] - 0s 631us/step - loss: 0.5613 - acc: 0.9000\n",
      "Epoch 79/100\n",
      "10/10 [==============================] - 0s 334us/step - loss: 0.5592 - acc: 0.9000\n",
      "Epoch 80/100\n",
      "10/10 [==============================] - 0s 451us/step - loss: 0.5571 - acc: 0.9000\n",
      "Epoch 81/100\n",
      "10/10 [==============================] - 0s 294us/step - loss: 0.5549 - acc: 0.9000\n",
      "Epoch 82/100\n",
      "10/10 [==============================] - 0s 313us/step - loss: 0.5528 - acc: 0.9000\n",
      "Epoch 83/100\n",
      "10/10 [==============================] - 0s 398us/step - loss: 0.5506 - acc: 0.9000\n",
      "Epoch 84/100\n",
      "10/10 [==============================] - 0s 602us/step - loss: 0.5484 - acc: 0.9000\n",
      "Epoch 85/100\n",
      "10/10 [==============================] - 0s 326us/step - loss: 0.5463 - acc: 0.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100\n",
      "10/10 [==============================] - 0s 430us/step - loss: 0.5441 - acc: 0.9000\n",
      "Epoch 87/100\n",
      "10/10 [==============================] - 0s 177us/step - loss: 0.5419 - acc: 0.9000\n",
      "Epoch 88/100\n",
      "10/10 [==============================] - 0s 145us/step - loss: 0.5397 - acc: 0.9000\n",
      "Epoch 89/100\n",
      "10/10 [==============================] - 0s 403us/step - loss: 0.5375 - acc: 0.9000\n",
      "Epoch 90/100\n",
      "10/10 [==============================] - 0s 252us/step - loss: 0.5353 - acc: 0.9000\n",
      "Epoch 91/100\n",
      "10/10 [==============================] - 0s 268us/step - loss: 0.5331 - acc: 0.9000\n",
      "Epoch 92/100\n",
      "10/10 [==============================] - 0s 169us/step - loss: 0.5309 - acc: 0.9000\n",
      "Epoch 93/100\n",
      "10/10 [==============================] - 0s 289us/step - loss: 0.5287 - acc: 0.9000\n",
      "Epoch 94/100\n",
      "10/10 [==============================] - 0s 173us/step - loss: 0.5265 - acc: 0.9000\n",
      "Epoch 95/100\n",
      "10/10 [==============================] - 0s 182us/step - loss: 0.5243 - acc: 0.9000\n",
      "Epoch 96/100\n",
      "10/10 [==============================] - 0s 466us/step - loss: 0.5221 - acc: 0.9000\n",
      "Epoch 97/100\n",
      "10/10 [==============================] - 0s 627us/step - loss: 0.5199 - acc: 0.9000\n",
      "Epoch 98/100\n",
      "10/10 [==============================] - 0s 162us/step - loss: 0.5177 - acc: 0.9000\n",
      "Epoch 99/100\n",
      "10/10 [==============================] - 0s 223us/step - loss: 0.5155 - acc: 0.9000\n",
      "Epoch 100/100\n",
      "10/10 [==============================] - 0s 278us/step - loss: 0.5133 - acc: 0.9000\n",
      "Accuracy: 89.999998\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=100, verbose=1, validation_split=0.0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.35821965]], dtype=float32)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(encode_input('not good'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_input('work!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 -0.48336 0.1464 -0.37304 0.34577 0.052041 0.44946 -0.46971 0.02628 -0.54155 -0.15518 -0.14107 -0.039722 0.28277 0.14393 0.23464 -0.31021 0.086173 0.20397 0.52624 0.17164 -0.082378 -0.71787 -0.41531 0.20335 -0.12763 0.41367 0.55187 0.57908 -0.33477 -0.36559 -0.54857 -0.062892 0.26584 0.30205 0.99775 -0.80481 -3.0243 0.01254 -0.36942 2.2167 0.72201 -0.24978 0.92136 0.034514 0.46745 1.1079 -0.19358 -0.074575 0.23353 -0.052062 -0.22044 0.057162 -0.15806 -0.30798 -0.41625 0.37972 0.15006 -0.53212 -0.2055 -1.2526 0.071624 0.70565 0.49744 -0.42063 0.26148 -1.538 -0.30223 -0.073438 -0.28312 0.37104 -0.25217 0.016215 -0.017099 -0.38984 0.87424 -0.72569 -0.51058 -0.52028 -0.1459 0.8278 0.27062\r\n"
     ]
    }
   ],
   "source": [
    "!head glove.6B.100d.txt -n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B.300d.txt')\n",
    "for line in f:\n",
    "\tvalues = line.split()\n",
    "\tword = values[0]\n",
    "\tcoefs = np.asarray(values[1:], dtype='float32')\n",
    "\tembeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "for word, i in t.word_index.items():\n",
    "\tembedding_vector = embeddings_index.get(word)\n",
    "\tif embedding_vector is not None:\n",
    "\t\tembedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_17 (InputLayer)        (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_27 (Embedding)     (None, 4, 300)            4500      \n",
      "_________________________________________________________________\n",
      "flatten_23 (Flatten)         (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 1201      \n",
      "=================================================================\n",
      "Total params: 5,701\n",
      "Trainable params: 1,201\n",
      "Non-trainable params: 4,500\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "inputs = Input(shape=(max_length,))\n",
    "embbeded_layer = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable=False)(inputs)\n",
    "flatten_layer = Flatten()(embbeded_layer)\n",
    "dense_layer = Dense(1, activation='sigmoid')(flatten_layer)\n",
    "model = Model(inputs=inputs,outputs=dense_layer)\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 1s 55ms/step - loss: 0.1876 - acc: 0.9000\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1777 - acc: 0.9000\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 0s 209us/step - loss: 0.1649 - acc: 1.0000\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 0s 230us/step - loss: 0.1511 - acc: 1.0000\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 0s 438us/step - loss: 0.1377 - acc: 1.0000\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 0s 148us/step - loss: 0.1255 - acc: 1.0000\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 0s 171us/step - loss: 0.1149 - acc: 1.0000\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 0s 146us/step - loss: 0.1060 - acc: 1.0000\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 0s 129us/step - loss: 0.0986 - acc: 1.0000\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 0s 147us/step - loss: 0.0926 - acc: 1.0000\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 0s 188us/step - loss: 0.0877 - acc: 1.0000\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 0s 249us/step - loss: 0.0836 - acc: 1.0000\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 0s 255us/step - loss: 0.0802 - acc: 1.0000\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 0s 193us/step - loss: 0.0774 - acc: 1.0000\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 0s 146us/step - loss: 0.0751 - acc: 1.0000\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 0s 236us/step - loss: 0.0731 - acc: 1.0000\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 0s 213us/step - loss: 0.0713 - acc: 1.0000\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 0s 134us/step - loss: 0.0698 - acc: 1.0000\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 0s 148us/step - loss: 0.0684 - acc: 1.0000\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 0s 308us/step - loss: 0.0672 - acc: 1.0000\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 0s 123us/step - loss: 0.0661 - acc: 1.0000\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 0s 128us/step - loss: 0.0651 - acc: 1.0000\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 0s 127us/step - loss: 0.0641 - acc: 1.0000\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 0s 525us/step - loss: 0.0631 - acc: 1.0000\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 0s 150us/step - loss: 0.0622 - acc: 1.0000\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 0s 124us/step - loss: 0.0614 - acc: 1.0000\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 0s 234us/step - loss: 0.0605 - acc: 1.0000\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 0s 206us/step - loss: 0.0596 - acc: 1.0000\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 0s 173us/step - loss: 0.0588 - acc: 1.0000\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 0s 123us/step - loss: 0.0580 - acc: 1.0000\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 0s 144us/step - loss: 0.0571 - acc: 1.0000\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - 0s 128us/step - loss: 0.0563 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - 0s 459us/step - loss: 0.0555 - acc: 1.0000\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - 0s 133us/step - loss: 0.0547 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - 0s 124us/step - loss: 0.0540 - acc: 1.0000\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - 0s 402us/step - loss: 0.0532 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - 0s 192us/step - loss: 0.0525 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "10/10 [==============================] - 0s 276us/step - loss: 0.0517 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "10/10 [==============================] - 0s 300us/step - loss: 0.0510 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "10/10 [==============================] - 0s 447us/step - loss: 0.0503 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "10/10 [==============================] - 0s 228us/step - loss: 0.0497 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "10/10 [==============================] - 0s 255us/step - loss: 0.0490 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "10/10 [==============================] - 0s 214us/step - loss: 0.0484 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "10/10 [==============================] - 0s 128us/step - loss: 0.0478 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "10/10 [==============================] - 0s 126us/step - loss: 0.0472 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "10/10 [==============================] - 0s 139us/step - loss: 0.0466 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "10/10 [==============================] - 0s 133us/step - loss: 0.0461 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "10/10 [==============================] - 0s 132us/step - loss: 0.0455 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "10/10 [==============================] - 0s 633us/step - loss: 0.0450 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "10/10 [==============================] - 0s 375us/step - loss: 0.0445 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "10/10 [==============================] - 0s 434us/step - loss: 0.0440 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "10/10 [==============================] - 0s 795us/step - loss: 0.0435 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "10/10 [==============================] - 0s 848us/step - loss: 0.0431 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "10/10 [==============================] - 0s 378us/step - loss: 0.0426 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "10/10 [==============================] - 0s 176us/step - loss: 0.0422 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "10/10 [==============================] - 0s 168us/step - loss: 0.0418 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0413 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "10/10 [==============================] - 0s 622us/step - loss: 0.0409 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "10/10 [==============================] - 0s 828us/step - loss: 0.0405 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "10/10 [==============================] - 0s 260us/step - loss: 0.0402 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "10/10 [==============================] - 0s 385us/step - loss: 0.0398 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "10/10 [==============================] - 0s 207us/step - loss: 0.0394 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "10/10 [==============================] - 0s 150us/step - loss: 0.0391 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "10/10 [==============================] - 0s 167us/step - loss: 0.0387 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "10/10 [==============================] - 0s 654us/step - loss: 0.0384 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "10/10 [==============================] - 0s 649us/step - loss: 0.0380 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "10/10 [==============================] - 0s 206us/step - loss: 0.0377 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "10/10 [==============================] - 0s 548us/step - loss: 0.0374 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "10/10 [==============================] - 0s 220us/step - loss: 0.0370 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "10/10 [==============================] - 0s 204us/step - loss: 0.0367 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "10/10 [==============================] - 0s 224us/step - loss: 0.0364 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "10/10 [==============================] - 0s 189us/step - loss: 0.0361 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "10/10 [==============================] - 0s 313us/step - loss: 0.0358 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "10/10 [==============================] - 0s 487us/step - loss: 0.0355 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "10/10 [==============================] - 0s 150us/step - loss: 0.0352 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "10/10 [==============================] - 0s 142us/step - loss: 0.0350 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "10/10 [==============================] - 0s 264us/step - loss: 0.0347 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "10/10 [==============================] - 0s 224us/step - loss: 0.0344 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "10/10 [==============================] - 0s 295us/step - loss: 0.0341 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "10/10 [==============================] - 0s 287us/step - loss: 0.0339 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "10/10 [==============================] - 0s 231us/step - loss: 0.0336 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "10/10 [==============================] - 0s 607us/step - loss: 0.0333 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "10/10 [==============================] - 0s 197us/step - loss: 0.0331 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "10/10 [==============================] - 0s 156us/step - loss: 0.0328 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "10/10 [==============================] - 0s 165us/step - loss: 0.0326 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100\n",
      "10/10 [==============================] - 0s 172us/step - loss: 0.0323 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "10/10 [==============================] - 0s 252us/step - loss: 0.0321 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "10/10 [==============================] - 0s 189us/step - loss: 0.0319 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "10/10 [==============================] - 0s 204us/step - loss: 0.0316 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "10/10 [==============================] - 0s 359us/step - loss: 0.0314 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "10/10 [==============================] - 0s 310us/step - loss: 0.0312 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "10/10 [==============================] - 0s 591us/step - loss: 0.0309 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "10/10 [==============================] - 0s 202us/step - loss: 0.0307 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "10/10 [==============================] - 0s 209us/step - loss: 0.0305 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "10/10 [==============================] - 0s 156us/step - loss: 0.0303 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "10/10 [==============================] - 0s 142us/step - loss: 0.0301 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "10/10 [==============================] - 0s 343us/step - loss: 0.0298 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "10/10 [==============================] - 0s 198us/step - loss: 0.0296 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "10/10 [==============================] - 0s 649us/step - loss: 0.0294 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "10/10 [==============================] - 0s 133us/step - loss: 0.0292 - acc: 1.0000\n",
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "# model = keras.models.load_model('word_embbeding.hd5')\n",
    "model.fit(padded_docs, labels, epochs=100, verbose=1, validation_split=0.1)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('word_embbeding.hd5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
